{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn2AsN_8OF9C"
      },
      "source": [
        "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{bloodmnist_esi_feb2023} -->\n",
        "\n",
        "Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\n",
        "\n",
        "\n",
        "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n",
        "\n",
        "\n",
        "## What this notebook covers with Weights and Biases:\n",
        "* Using W&B to save and download your data\n",
        "* Exploratory Data Analysis (EDA)\n",
        "* Metrics logging "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNu4MLLEWoGm"
      },
      "source": [
        "# ‚úÖ Sign Up\n",
        "\n",
        "Sign up to a free [Weights & Biases account here](https://wandb.ai/signup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEjqFP8xuZ2g"
      },
      "source": [
        "[Weights and Biases docs](https://docs.wandb.ai/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vAHpRh9hi2"
      },
      "source": [
        "# Kaggle Competition Page\n",
        "\n",
        "[Submit to the Competition here](https://www.kaggle.com/t/e77aca50e0bc4317bda513775a606aee)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmx5yiSHodB"
      },
      "source": [
        "# üöÄ Installing and importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32x5NdqZE2e1"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade wandb\n",
        "!pip install -qq timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQV_RY7nauWI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import timm\n",
        "import wandb\n",
        "import random\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK-8urUht2VY"
      },
      "source": [
        "Set some constants "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECCOZJy_Lx5O"
      },
      "outputs": [],
      "source": [
        "PROJECT = 'blood-mnist-esi-feb2023'\n",
        "DATA_DIR = 'data'\n",
        "ARTIFACT_PATH = 'wandb_fc/blood_mnist/blood_mnist_data:latest'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHlJ_rz_Izr7"
      },
      "source": [
        "# üíæ Data\n",
        "#### Download the data from W&B Artifacts\n",
        "Train, validation and test images will be downloaded, as well as train and validation labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxu2Zl3wHpcU"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=PROJECT, job_type='download_dataset')\n",
        "\n",
        "artifact = wandb.use_artifact(ARTIFACT_PATH, type='competition_dataset')\n",
        "\n",
        "artifact_dir = artifact.download(DATA_DIR)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaGaWjsSsm6Q"
      },
      "source": [
        "## Prepare the Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tljoyl8nbkCS"
      },
      "outputs": [],
      "source": [
        "class BloodMNISTImageDataset(Dataset):\n",
        "    def __init__(self, image_path, labels_path=None, transforms=None):\n",
        "        self.data = torch.load(image_path)\n",
        "        if labels_path is not None:\n",
        "          self.labels = torch.load(labels_path)\n",
        "        else:\n",
        "          self.labels = None\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx].permute(2,0,1)\n",
        "        img = self.transforms(img)\n",
        "        if self.labels is not None:\n",
        "          return img, self.labels[idx].long()\n",
        "        else:\n",
        "          return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV8luZPgsxpq"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(dataset, is_train, batch_size):\n",
        "    \"Get a dataloader\"\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
        "                        shuffle=True if is_train else False, \n",
        "                        pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
        "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
        "    model.eval()\n",
        "    val_loss = 0.\n",
        "    with torch.inference_mode():\n",
        "        correct = 0\n",
        "        for i, (images, labels) in tqdm(enumerate(valid_dl), leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass ‚û°\n",
        "            outputs = model(images)\n",
        "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
        "\n",
        "            # Compute accuracy and accumulate\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Log validation predictions and images to the dashboard\n",
        "            if log_images:\n",
        "              if i ==0:\n",
        "                # üêù Create a wandb Table to log images, labels and predictions to\n",
        "                table = wandb.Table(columns=[\"image\", \"label\", \"pred\"]+[f\"score_{i}\" for i in range(8)])\n",
        "              \n",
        "              probs = outputs.softmax(dim=1)\n",
        "              for img, label, pred, prob in zip(images.to(\"cpu\"), labels.to(\"cpu\"), predicted.to(\"cpu\"),  probs.to(\"cpu\")):\n",
        "                  # table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
        "                  table.add_data(wandb.Image(img[0].numpy()), label, pred, *prob.numpy())\n",
        "        \n",
        "        if log_images:\n",
        "          wandb.log({\"val_table/predictions_table\":table}, commit=False)\n",
        "\n",
        "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v6_lwPtYPSd"
      },
      "source": [
        "Initialise the Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0QTt8NxxOBd"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_tfms = T.Compose([\n",
        "  # T.CenterCrop(10),\n",
        "  T.ConvertImageDtype(torch.float),\n",
        " ])\n",
        "\n",
        "val_tfms = T.Compose([\n",
        "  T.ConvertImageDtype(torch.float),\n",
        " ])\n",
        "\n",
        "train_dataset = BloodMNISTImageDataset(f'{DATA_DIR}/train_data.pt', f'{DATA_DIR}/train_labels.pt', transforms=train_tfms)\n",
        "valid_dataset = BloodMNISTImageDataset(f'{DATA_DIR}/val_data.pt', f'{DATA_DIR}/val_labels.pt', transforms=val_tfms)\n",
        "test_dataset = BloodMNISTImageDataset(f'{DATA_DIR}/test_data.pt', transforms=val_tfms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjlnqjNzzBnB"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][0].size(), valid_dataset[0][0].size(), test_dataset[0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y34PO0JtcET2"
      },
      "source": [
        "Initialise the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgKeCaPNxeZ1"
      },
      "outputs": [],
      "source": [
        "BS = 128\n",
        "\n",
        "train_dl = get_dataloader(train_dataset, is_train=True, batch_size=BS)\n",
        "valid_dl = get_dataloader(valid_dataset, is_train=False, batch_size=BS)\n",
        "test_dl = get_dataloader(test_dataset, is_train=False, batch_size=BS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QitMqcHzRjTr"
      },
      "outputs": [],
      "source": [
        "len(train_dl), len(valid_dl) , len(test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZfOpabjL4kf"
      },
      "source": [
        "# üñºÔ∏è EDA with W&B Tables\n",
        "Log the train and validation datasets to W&B Tables for EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD_7X40vHpQ0"
      },
      "outputs": [],
      "source": [
        "# Create a W&B run\n",
        "wandb.init(project=PROJECT, job_type='log_dataset')\n",
        "\n",
        "# üêù Create a wandb Table to log images, labels and predictions to\n",
        "table = wandb.Table(columns=[\"image\", \"label\"])\n",
        "\n",
        "for step, (images, labels) in enumerate(tqdm(train_dl, leave=False)):\n",
        "  for img, label in zip(images.to(\"cpu\"), labels.to(\"cpu\")):\n",
        "    w_img = wandb.Image(img[0].numpy())\n",
        "    table.add_data(w_img, label)\n",
        "  \n",
        "  if step == 10: \n",
        "    break\n",
        "\n",
        "wandb.log({'train_data/train_table':table})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5ni9t9sfArq"
      },
      "source": [
        "#üëü Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgppQ0msCglo"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3\n",
        "EPOCHS = 3\n",
        "\n",
        "# Log the final results on the validation set\n",
        "LOG_IMAGES=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqWMZCd1xfIJ"
      },
      "source": [
        "Get Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f6TJigHwcke"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'cspresnet50d'\n",
        "\n",
        "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=8)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIU3VCweTvSX"
      },
      "source": [
        "üêù initialise a wandb run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSvGEtymTtMB"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=PROJECT, config={\"epochs\": EPOCHS, \"batch_size\": BS, \"lr\": LR})\n",
        "\n",
        "# Add additional configs to wandb if needed\n",
        "wandb.config['len_train'] = len(train_dataset)\n",
        "wandb.config['len_val'] = len(valid_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLS60o7FDk6L"
      },
      "source": [
        "Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqD2_42JxFwO"
      },
      "outputs": [],
      "source": [
        "# Copy your config \n",
        "config = wandb.config\n",
        "\n",
        "# Get the data\n",
        "\n",
        "\n",
        "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "\n",
        "# Make the loss and optimizer\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "# Training\n",
        "example_ct = 0\n",
        "step_ct = 0\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    model.train()\n",
        "    for step, (images, labels) in enumerate(tqdm(train_dl, leave=False)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        train_loss = loss_func(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        example_ct += len(images)\n",
        "        metrics = {\"train/train_loss\": train_loss, \n",
        "                    \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
        "                    \"train/example_ct\": example_ct}\n",
        "        \n",
        "        if step + 1 < n_steps_per_epoch:\n",
        "            # üêù Log train metrics to wandb \n",
        "            wandb.log(metrics)\n",
        "            \n",
        "        step_ct += 1\n",
        "    \n",
        "    # log validation images and predictions on last epoch\n",
        "    if LOG_IMAGES:\n",
        "      log_images = epoch==(config.epochs-1)\n",
        "    else:\n",
        "      log_images = False\n",
        "\n",
        "    # Do validation and maybe log images to Tables\n",
        "    val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=log_images)\n",
        "\n",
        "    # üêù Log train and validation metrics to wandb\n",
        "    val_metrics = {\"val/val_loss\": val_loss, \n",
        "                    \"val/val_accuracy\": accuracy}\n",
        "    wandb.log({**metrics, **val_metrics})\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Valid Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Save trained model to disk and to W&B Artifacts\n",
        "model_fn = f'{MODEL_NAME}_model.pt'\n",
        "torch.save(model, model_fn)\n",
        "wandb.log_artifact(model_fn, f'{MODEL_NAME}_model', type='model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DesBwID32HJ"
      },
      "source": [
        "# Generate Test Submission\n",
        "Generate Test Predictions and Log Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjo2pIZcRMKA"
      },
      "outputs": [],
      "source": [
        "preds = []\n",
        "for step, images in enumerate(tqdm(test_dl, leave=False)):\n",
        "  images = images.to(device)\n",
        "  outputs = model(images)\n",
        "  preds.append(outputs.argmax(1).cpu().numpy().tolist())\n",
        "\n",
        "preds = [p for ps in preds for p in ps]\n",
        "len(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ9mZZyCUDtL"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame({'Id':list(range(len(preds))), 'Category':preds})\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEwjSJqjUMeF"
      },
      "source": [
        "Log your submission file to your wandb run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu0Lu92KTYqf"
      },
      "outputs": [],
      "source": [
        "wandb.log_artifact('submission.csv', 'submission_file', type='submission')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C03T4ecUK-H"
      },
      "source": [
        "üêù Close your wandb run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHtZ-QEuVV8"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A4CVu1L1zN0"
      },
      "source": [
        "# ü™Ñ More from W&B\n",
        "\n",
        "#### üìè Best Practices\n",
        "\n",
        "1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n",
        "2. **Groups**: For multiple processes or cross validation folds, log each process as a run and group them together. `wandb.init(group='experiment-1')`\n",
        "3. **Tags**: Add tags to track your current baseline or production model.\n",
        "4. **Notes**: Type notes in the table to track the changes between runs.\n",
        "5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.\n",
        "\n",
        "# What's next üöÄ ?\n",
        "The next tutorial you will learn how to do hyperparameter optimization using W&B Sweeps:\n",
        "## üëâ [Hyperparameters sweeps using PyTorch](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}